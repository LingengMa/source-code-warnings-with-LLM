#!/usr/bin/env python3
"""
åˆ‡ç‰‡åˆ†æ + ä»£ç é‡æ„ - ç”Ÿæˆå®Œæ•´çš„åˆ‡ç‰‡ç»“æœï¼ˆåŒ…å«é‡æ„ä»£ç ï¼‰
"""

import os
import json
import argparse
import threading
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from slice_analyzer import CSemanticSlicer
from slice_reconstructor import reconstruct_slice

# å°è¯•å¯¼å…¥æ›´å¿«çš„JSONåº“
try:
    import orjson
    USE_ORJSON = True
except ImportError:
    USE_ORJSON = False

# å¿«é€ŸJSONåºåˆ—åŒ–è¾…åŠ©å‡½æ•°
def fast_json_dump(data, file_path):
    """ä½¿ç”¨æœ€å¿«çš„å¯ç”¨JSONåº“ä¿å­˜æ•°æ®"""
    if USE_ORJSON:
        # orjson æœ€å¿«ï¼Œä½†è¿”å›bytes
        with open(file_path, 'wb') as f:
            f.write(orjson.dumps(data, option=orjson.OPT_INDENT_2))
    else:
        # æ ‡å‡†åº“ï¼ˆæœ€æ…¢ï¼‰
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)


def fast_json_load(file_path):
    """ä½¿ç”¨æœ€å¿«çš„å¯ç”¨JSONåº“åŠ è½½æ•°æ®"""
    if USE_ORJSON:
        with open(file_path, 'rb') as f:
            return orjson.loads(f.read())
    else:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)


# çº¿ç¨‹é”ç”¨äºä¿æŠ¤å…±äº«èµ„æº
print_lock = threading.Lock()
results_lock = threading.Lock()


def process_with_reconstruction(file_path: str, target_line: int,
                                enable_interprocedural: bool = True,
                                max_call_depth: int = 1,
                                verbose: bool = False) -> dict:
    """
    æ‰§è¡Œåˆ‡ç‰‡åˆ†æå¹¶é‡æ„ä»£ç 
    
    Returns:
        åŒ…å«åˆ‡ç‰‡ä¿¡æ¯å’Œé‡æ„ä»£ç çš„å®Œæ•´ç»“æœ
    """
    # è¯»å–æºä»£ç 
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        source_code = f.read()
    
    # åˆ¤æ–­æ–‡ä»¶ç±»å‹
    is_cpp = file_path.endswith(('.cpp', '.cc', '.cxx', '.hpp', '.hxx'))
    
    # æ‰§è¡Œåˆ‡ç‰‡åˆ†æï¼ˆä»…verboseæ¨¡å¼æ‰“å°ï¼‰
    if verbose:
        print(f"åˆ†ææ–‡ä»¶: {file_path}")
        print(f"ç›®æ ‡è¡Œ: {target_line}")
    
    slicer = CSemanticSlicer(is_cpp=is_cpp)
    slice_result = slicer.slice(
        file_path, target_line,
        enable_interprocedural=enable_interprocedural,
        max_call_depth=max_call_depth
    )
    
    if not slice_result:
        return {
            'error': 'Slice analysis failed',
            'file': file_path,
            'target_line': target_line,
            'reconstruction_success': False
        }
    
    # å‡†å¤‡åŸºç¡€ç»“æœ
    result = slice_result.to_dict()
    result['file'] = file_path
    result['config'] = {
        'interprocedural': enable_interprocedural,
        'max_call_depth': max_call_depth
    }
    
    # æå–åˆ‡ç‰‡å¯¹åº”çš„åŸå§‹ä»£ç è¡Œ
    source_lines = source_code.split('\n')
    slice_code_lines = []
    for line_num in sorted(slice_result.slice_lines):
        if 0 < line_num <= len(source_lines):
            slice_code_lines.append({
                'line_number': line_num,
                'code': source_lines[line_num - 1],
                'function': result['function_map'].get(str(line_num), 'unknown')
            })
    
    result['slice_code_lines'] = slice_code_lines
    result['slice_size'] = len(slice_result.slice_lines)
    
    # æ·»åŠ ç›®æ ‡è¡Œæ‰€åœ¨å‡½æ•°
    result['target_function'] = result['function_map'].get(str(target_line), 'unknown')
    
    # æ‰§è¡Œä»£ç é‡æ„
    if verbose:
        print("é‡æ„åˆ‡ç‰‡ä»£ç ...")
    try:
        reconstructed_code = reconstruct_slice(source_code, result)
        result['reconstructed_code'] = reconstructed_code
        result['reconstruction_success'] = True
    except Exception as e:
        if verbose:
            print(f"é‡æ„å¤±è´¥: {e}")
        result['reconstructed_code'] = f"/* Reconstruction failed: {str(e)} */"
        result['reconstruction_success'] = False
    
    # æ·»åŠ é¢å¤–åˆ†æä¿¡æ¯
    result['analysis_info'] = {
        'pointer_aliases': {k: list(v) for k, v in slicer.pointer_aliases.items()},
        'function_info': {},
        'type_info': slicer.type_info,
        'call_graph': {k: list(v) for k, v in slicer.call_graph.items()}
    }
    
    # å‡½æ•°ä¿¡æ¯
    for func_name, func_info in slicer.function_info.items():
        result['analysis_info']['function_info'][func_name] = {
            'params': func_info.params,
            'pointer_params': list(func_info.pointer_params),
            'return_type': func_info.return_type,
            'modifies_globals': list(func_info.modifies_globals),
            'may_modify_params': list(func_info.may_modify_params),
            'is_recursive': func_info.is_recursive
        }
    
    return result


def sort_results_by_dataset_order(results: list, dataset: list) -> list:
    """æŒ‰æ•°æ®é›†åŸå§‹é¡ºåºæ’åºç»“æœ"""
    # åˆ›å»ºä¸€ä¸ªæ˜ å°„æ¥æ¢å¤é¡ºåº
    result_map = {}
    for result in results:
        key = (result.get('project'), 
               result.get('file', result.get('target_file', '')).split('/')[-1] 
               if '/' in result.get('file', result.get('target_file', '')) 
               else result.get('file', result.get('target_file', '')),
               result.get('target_line'))
        result_map[key] = result
    
    # æŒ‰æ•°æ®é›†é¡ºåºé‡æ–°ç»„ç»‡
    sorted_results = []
    for entry in dataset:
        project = entry['project_name_with_version'] # ä½¿ç”¨ project_name_with_version å­—æ®µä½œä¸ºé¡¹ç›®ç›®å½•
        file_rel = entry['file_path']
        target_line = entry['line_number']
        
        # å°è¯•å¤šç§é”®åŒ¹é…
        result = None
        for key in result_map:
            if (key[0] == project and key[2] == target_line and 
                (key[1] in file_rel or file_rel in key[1])):
                result = result_map[key]
                break
        
        if result:
            sorted_results.append(result)
    
    return sorted_results if sorted_results else results


def process_single_entry(entry: dict, idx: int, total: int,
                        enable_interprocedural: bool = True,
                        max_call_depth: int = 1,
                        verbose: bool = False) -> dict:
    """å¤„ç†å•ä¸ªæ•°æ®é›†æ¡ç›®ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    project_name = entry['project_name_with_version'] # ä½¿ç”¨ project_name_with_version å­—æ®µä½œä¸ºé¡¹ç›®ç›®å½•
    file_path_rel = entry['file_path']
    target_line = entry['line_number']
    
    # æ„å»ºå®Œæ•´è·¯å¾„
    full_path = os.path.join('input/repository', project_name, file_path_rel)
    
    # åªåœ¨verboseæ¨¡å¼ä¸‹æ‰“å°è¯¦ç»†ä¿¡æ¯
    if verbose:
        with print_lock:
            print(f"\n{'='*60}")
            print(f"[{idx+1}/{total}] Processing...")
            print(f"  æ–‡ä»¶: {file_path_rel}")
            print(f"  è¡Œå·: {target_line}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(full_path):
        if verbose:
            with print_lock:
                print(f"  âœ— æ–‡ä»¶ä¸å­˜åœ¨: {full_path}") # æ‰“å°å®Œæ•´è·¯å¾„ä»¥ä¾›è°ƒè¯•
        return {
            'project': project_name,
            'file': file_path_rel,
            'target_line': target_line,
            'error': 'file_not_found',
            'reconstruction_success': False
        }
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºC/C++æ–‡ä»¶
    c_cpp_extensions = ('.c', '.cc', '.cpp', '.cxx', '.h', '.hpp', '.hxx')
    if not full_path.lower().endswith(c_cpp_extensions):
        if verbose:
            with print_lock:
                print(f"  âš  è·³è¿‡éC/C++æ–‡ä»¶")
        return {
            'project': project_name,
            'file': file_path_rel,
            'target_line': target_line,
            'error': 'not_c_cpp_file',
            'reconstruction_success': False
        }
    
    # æ‰§è¡Œåˆ†æå’Œé‡æ„
    try:
        result = process_with_reconstruction(
            full_path, target_line,
            enable_interprocedural=enable_interprocedural,
            max_call_depth=max_call_depth,
            verbose=verbose
        )
        
        result['project'] = project_name
        
        if verbose:
            with print_lock:
                print(f"  âœ“ åˆ‡ç‰‡å¤§å°: {result.get('slice_size', 0)} è¡Œ")
                print(f"  âœ“ é‡æ„: {'æˆåŠŸ' if result.get('reconstruction_success') else 'å¤±è´¥'}")
        
        return result
        
    except Exception as e:
        if verbose:
            with print_lock:
                print(f"  âœ— å¤„ç†å¤±è´¥: {e}")
        return {
            'project': project_name,
            'file': file_path_rel,
            'target_line': target_line,
            'error': str(e),
            'reconstruction_success': False
        }


def process_dataset(data_file: str, output_file: str,
                   enable_interprocedural: bool = True,
                   max_call_depth: int = 1,
                   max_samples: int = None,
                   resume: bool = True,
                   num_workers: int = None,
                   save_interval: int = 100,
                   use_processes: bool = True,
                   chunk_size: int = 1000):
    """å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ã€å¤šè¿›ç¨‹/å¤šçº¿ç¨‹å’Œå®šæœŸä¿å­˜ï¼‰
    
    Args:
        use_processes: Trueä½¿ç”¨å¤šè¿›ç¨‹ï¼ˆCPUå¯†é›†å‹æ¨èï¼‰ï¼ŒFalseä½¿ç”¨å¤šçº¿ç¨‹ï¼ˆI/Oå¯†é›†å‹ï¼‰
        chunk_size: åˆ†å—ä¿å­˜å¤§å°ï¼Œæ¯Næ¡ç»“æœä¿å­˜åˆ°ä¸€ä¸ªå•ç‹¬çš„chunkæ–‡ä»¶
    """
    # è‡ªåŠ¨ç¡®å®šworkeræ•°é‡
    if num_workers is None:
        if use_processes:
            num_workers = multiprocessing.cpu_count()
        else:
            num_workers = multiprocessing.cpu_count() * 2
    
    # è¯»å–æ•°æ®é›†
    dataset = fast_json_load(data_file)
    
    # åˆ›å»ºchunksç›®å½•
    chunks_dir = os.path.join(os.path.dirname(output_file) or '.', 'chunks')
    os.makedirs(chunks_dir, exist_ok=True)
    
    # åŠ è½½å·²æœ‰ç»“æœï¼ˆæ–­ç‚¹ç»­ä¼  - æ”¯æŒchunksï¼‰
    existing_results = {}
    if resume:
        try:
            # ä¼˜å…ˆä»chunksç›®å½•åŠ è½½ï¼ˆæœ€æ–°æ•°æ®ï¼‰
            if os.path.exists(chunks_dir):
                chunk_files = sorted([f for f in os.listdir(chunks_dir) if f.startswith('success_chunk_')])
                if chunk_files:
                    print(f"ğŸ“‚ ä»chunksç›®å½•åŠ è½½æ–­ç‚¹æ•°æ®...")
                    for chunk_file in chunk_files:
                        chunk_path = os.path.join(chunks_dir, chunk_file)
                        chunk_data = fast_json_load(chunk_path)
                        for result in chunk_data:
                            project = result.get('project')
                            target_line = result.get('target_line')
                            
                            # æå–ç›¸å¯¹è·¯å¾„
                            file_path = result.get('file', result.get('target_file', ''))
                            if file_path.startswith('input/repository/'):
                                parts = file_path.split('/', 3)
                                if len(parts) >= 4:
                                    file_path = parts[3]
                            
                            key = (project, file_path, target_line)
                            
                            # åªä¿ç•™æˆåŠŸçš„ç»“æœ
                            if result.get('reconstruction_success', False):
                                existing_results[key] = result
                    print(f"   å·²åŠ è½½: {len(existing_results)} ä¸ªæˆåŠŸæ¡ç›®")
            
            # å¦‚æœchunksç›®å½•ä¸ºç©ºï¼Œå°è¯•ä»æœ€ç»ˆè¾“å‡ºæ–‡ä»¶åŠ è½½
            if not existing_results and os.path.exists(output_file):
                print(f"ğŸ“‚ ä»è¾“å‡ºæ–‡ä»¶åŠ è½½æ–­ç‚¹æ•°æ®...")
                existing = fast_json_load(output_file)
                for result in existing:
                    project = result.get('project')
                    target_line = result.get('target_line')
                    
                    # æå–ç›¸å¯¹è·¯å¾„
                    file_path = result.get('file', result.get('target_file', ''))
                    if file_path.startswith('input/repository/'):
                        parts = file_path.split('/', 3)
                        if len(parts) >= 4:
                            file_path = parts[3]
                    
                    key = (project, file_path, target_line)
                    
                    # åªä¿ç•™æˆåŠŸçš„ç»“æœ
                    if result.get('reconstruction_success', False):
                        existing_results[key] = result
                
                print(f"   å·²åŠ è½½: {len(existing_results)} ä¸ªæˆåŠŸæ¡ç›®")
        except Exception as e:
            print(f"âš  æ— æ³•åŠ è½½å·²æœ‰ç»“æœ: {e}")
    
    # åˆ†ç¦»å·²å¤„ç†å’Œå¾…å¤„ç†çš„æ¡ç›®
    to_process = []
    results = []
    skipped = 0
    
    for idx, entry in enumerate(dataset):
        project_name = entry['project_name_with_version'] # ä½¿ç”¨ project_name_with_version å­—æ®µä½œä¸ºé¡¹ç›®ç›®å½•
        file_path_rel = entry['file_path']
        target_line = entry['line_number']
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
        key = (project_name, file_path_rel, target_line)
        if key in existing_results:
            results.append(existing_results[key])
            skipped += 1
        else:
            to_process.append((idx, entry))
    
    print(f"\nğŸ“Š ä»»åŠ¡ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(dataset)}")
    print(f"  å·²å®Œæˆ: {skipped}")
    print(f"  å¾…å¤„ç†: {len(to_process)}")
    print(f"  å¹¶å‘æ¨¡å¼: {'å¤šè¿›ç¨‹' if use_processes else 'å¤šçº¿ç¨‹'} Ã— {num_workers}")
    print(f"  è‡ªåŠ¨ä¿å­˜: æ¯ {save_interval} ä¸ªæ ·æœ¬")
    print(f"  åˆ†å—ä¿å­˜: æ¯ {chunk_size} æ¡/æ–‡ä»¶")
    
    # åˆ›å»ºchunksç›®å½•
    chunks_dir = os.path.join(os.path.dirname(output_file) or '.', 'chunks')
    os.makedirs(chunks_dir, exist_ok=True)
    
    if not to_process:
        print(f"\nâœ“ æ‰€æœ‰æ ·æœ¬å·²å¤„ç†å®Œæˆ!")
    else:
        # ä½¿ç”¨è¿›ç¨‹æ± æˆ–çº¿ç¨‹æ± å¤„ç†
        executor_class = ProcessPoolExecutor if use_processes else ThreadPoolExecutor
        print(f"\nğŸš€ å¼€å§‹{'å¤šè¿›ç¨‹' if use_processes else 'å¤šçº¿ç¨‹'}å¤„ç†...\n")
        
        # å®šæœŸä¿å­˜é…ç½®
        last_save_count = 0
        
        def save_intermediate_results():
            """ä¿å­˜ä¸­é—´ç»“æœï¼ˆåˆ†å—ä¿å­˜ä¼˜åŒ–ç‰ˆï¼‰"""
            with print_lock:
                print(f"\nğŸ’¾ åˆ†å—ä¿å­˜ä¸­ ({len(results)} æ¡)...", end='', flush=True)
            
            # å¿«é€Ÿåˆ†ç¦»ï¼ˆä¸æ’åºï¼‰
            with results_lock:
                success_list = []
                failed_list = []
                for result in results:
                    simplified = {k: v for k, v in result.items() 
                                       if k not in ['function_map', 'slice_code_lines', 'analysis_info']}
                    if result.get('reconstruction_success', False):
                        success_list.append(simplified)
                    else:
                        failed_list.append(simplified)
            
            # åˆ†å—ä¿å­˜æˆåŠŸç»“æœ
            chunk_id = 0
            for i in range(0, len(success_list), chunk_size):
                chunk = success_list[i:i+chunk_size]
                chunk_file = os.path.join(chunks_dir, f'success_chunk_{chunk_id:04d}.json')
                fast_json_dump(chunk, chunk_file)
                chunk_id += 1
            
            # ä¿å­˜å¤±è´¥ç»“æœï¼ˆé€šå¸¸å¾ˆå°‘ï¼Œä¸åˆ†å—ï¼‰
            if failed_list:
                failed_file = os.path.join(chunks_dir, 'failed_all.json')
                fast_json_dump(failed_list, failed_file)
            
            with print_lock:
                print(f" å®Œæˆ! ({chunk_id} ä¸ªchunk, æˆåŠŸ:{len(success_list)}, å¤±è´¥:{len(failed_list)})")
        
        with executor_class(max_workers=num_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡ï¼ˆç¦ç”¨verboseå‡å°‘è¾“å‡ºï¼‰
            future_to_idx = {}
            for idx, entry in to_process:
                future = executor.submit(
                    process_single_entry,
                    entry, idx, len(dataset),
                    enable_interprocedural, max_call_depth,
                    False  # verbose=False æå‡æ€§èƒ½
                )
                future_to_idx[future] = idx
            
            # æ”¶é›†ç»“æœï¼ˆæŒ‰å®Œæˆé¡ºåºï¼‰
            completed = 0
            for future in as_completed(future_to_idx):
                idx = future_to_idx[future]
                try:
                    result = future.result()
                    with results_lock:
                        results.append(result)
                    completed += 1
                    
                    # åªåœ¨ç‰¹å®šé‡Œç¨‹ç¢‘æˆ–ä¿å­˜æ—¶æ‰“å°
                    should_print = (completed % 50 == 0 or 
                                  completed == len(to_process) or
                                  completed - last_save_count >= save_interval)
                    
                    if should_print:
                        with print_lock:
                            print(f"\nğŸ“ˆ è¿›åº¦: {completed}/{len(to_process)} å®Œæˆ (æ€»è®¡: {len(results)} æ¡)")
                    
                    # å®šæœŸä¿å­˜
                    if completed - last_save_count >= save_interval:
                        save_intermediate_results()
                        last_save_count = completed
                        
                except Exception as e:
                    with print_lock:
                        print(f"\nâœ— ä»»åŠ¡ {idx+1} å¼‚å¸¸: {e}")
                    with results_lock:
                        results.append({
                            'project': dataset[idx]['project_name_with_version'], # ä½¿ç”¨ project_name_with_version å­—æ®µ
                            'file': dataset[idx]['file_path'],
                            'target_line': dataset[idx]['line_number'],
                            'error': f'thread_exception: {str(e)}',
                            'reconstruction_success': False
                        })
    
    # æœ€ç»ˆä¿å­˜ï¼šåˆå¹¶æ‰€æœ‰chunkså¹¶æ’åº
    print(f"\nğŸ”„ åˆå¹¶chunkså¹¶æ’åº...")
    
    # è¯»å–æ‰€æœ‰æˆåŠŸçš„chunks
    all_success = []
    chunk_files = sorted([f for f in os.listdir(chunks_dir) if f.startswith('success_chunk_')])
    for chunk_file in chunk_files:
        chunk_path = os.path.join(chunks_dir, chunk_file)
        chunk_data = fast_json_load(chunk_path)
        all_success.extend(chunk_data)
    
    # è¯»å–å¤±è´¥ç»“æœ
    all_failed = []
    failed_path = os.path.join(chunks_dir, 'failed_all.json')
    if os.path.exists(failed_path):
        all_failed = fast_json_load(failed_path)
    
    # åˆå¹¶å¹¶æŒ‰åŸå§‹é¡ºåºæ’åº
    all_results = all_success + all_failed
    
    # é‡æ„resultsåˆ—è¡¨ç”¨äºæ’åº
    results_for_sort = []
    for r in all_results:
        # è¿˜åŸå®Œæ•´ç»“æ„ï¼ˆæ·»åŠ å›è¢«åˆ é™¤çš„å­—æ®µï¼Œç”¨äºæ’åºï¼‰
        full_result = r.copy()
        results_for_sort.append(full_result)
    
    sorted_results = sort_results_by_dataset_order(results_for_sort, dataset)
    
    # å†æ¬¡åˆ†ç¦»
    success_results = []
    failed_results = []
    for result in sorted_results:
        simplified = {k: v for k, v in result.items() 
                     if k not in ['function_map', 'slice_code_lines', 'analysis_info']}
        if result.get('reconstruction_success', False):
            success_results.append(simplified)
        else:
            failed_results.append(simplified)
    
    # ä¿å­˜æœ€ç»ˆåˆå¹¶ç»“æœ
    fast_json_dump(success_results, output_file)
    
    if failed_results:
        failed_file = output_file.replace('.json', '_failed.json')
        fast_json_dump(failed_results, failed_file)
        print(f"âš ï¸  å¤±è´¥ç»“æœ: {failed_file}")
    
    print(f"\n{'='*60}")
    print(f"âœ… å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"   Chunksç›®å½•: {chunks_dir} (å¯åˆ é™¤)")
    print(f"æ€»è®¡: {len(dataset)} ä¸ªæ ·æœ¬")
    print(f"è·³è¿‡: {skipped} ä¸ªï¼ˆå·²æœ‰æˆåŠŸç»“æœï¼‰")
    print(f"æœ¬æ¬¡å¤„ç†: {len(to_process)} ä¸ª")
    print(f"æ€»æˆåŠŸ: {len(success_results)} ä¸ª")
    print(f"æ€»å¤±è´¥: {len(failed_results)} ä¸ª")
    if USE_ORJSON:
        print(f"JSONåº“: orjson (æœ€å¿«)")
    else:
        print(f"JSONåº“: æ ‡å‡†åº“ (å»ºè®®: pip install orjson)")


def main():
    parser = argparse.ArgumentParser(
        description='åˆ‡ç‰‡åˆ†æ + ä»£ç é‡æ„ - ç”Ÿæˆå®Œæ•´ç»“æœ'
    )
    parser.add_argument('--data-file', type=str, default='input/data.json',
                       help='è¾“å…¥æ•°æ®æ–‡ä»¶')
    parser.add_argument('--output', '-o', type=str, default='output/results.json',
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--no-interprocedural', action='store_true',
                       help='ç¦ç”¨è¿‡ç¨‹é—´åˆ†æ')
    parser.add_argument('--max-call-depth', type=int, default=1,
                       help='æœ€å¤§è°ƒç”¨æ·±åº¦')
    parser.add_argument('--max-samples', type=int, default=None,
                       help='æœ€å¤§å¤„ç†æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--no-resume', action='store_true',
                       help='ç¦ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆé‡æ–°å¤„ç†æ‰€æœ‰æ ·æœ¬ï¼‰')
    parser.add_argument('--workers', '-j', type=int, default=None,
                       help='å¹¶å‘workeræ•°é‡ï¼ˆé»˜è®¤: å¤šè¿›ç¨‹=CPUæ ¸å¿ƒæ•°ï¼Œå¤šçº¿ç¨‹=CPUæ ¸å¿ƒæ•°Ã—2ï¼‰')
    parser.add_argument('--use-threads', action='store_true',
                       help='ä½¿ç”¨å¤šçº¿ç¨‹è€Œéå¤šè¿›ç¨‹ï¼ˆä¸æ¨èï¼Œä»…ç”¨äºI/Oå¯†é›†å‹ä»»åŠ¡ï¼‰')
    parser.add_argument('--save-interval', type=int, default=100,
                       help='è‡ªåŠ¨ä¿å­˜é—´éš”ï¼ˆå¤„ç†Nä¸ªæ ·æœ¬åä¿å­˜ï¼Œé»˜è®¤: 100ï¼‰')
    parser.add_argument('--chunk-size', type=int, default=1000,
                       help='åˆ†å—ä¿å­˜å¤§å°ï¼ˆæ¯Næ¡ç»“æœä¿å­˜åˆ°ä¸€ä¸ªchunkæ–‡ä»¶ï¼Œé»˜è®¤: 1000ï¼‰')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è¾“å‡ºè¯¦ç»†å¤„ç†ä¿¡æ¯ï¼ˆä¼šé™ä½æ€§èƒ½ï¼‰')
    
    # å•æ–‡ä»¶æ¨¡å¼
    parser.add_argument('--file', type=str,
                       help='å•ä¸ªæ–‡ä»¶è·¯å¾„ï¼ˆå•æ–‡ä»¶æ¨¡å¼ï¼‰')
    parser.add_argument('--line', type=int,
                       help='ç›®æ ‡è¡Œå·ï¼ˆå•æ–‡ä»¶æ¨¡å¼ï¼‰')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(args.output) or '.', exist_ok=True)
    
    if args.file and args.line:
        # å•æ–‡ä»¶æ¨¡å¼
        print("=" * 60)
        print("å•æ–‡ä»¶åˆ‡ç‰‡ + é‡æ„æ¨¡å¼")
        print("=" * 60)
        
        result = process_with_reconstruction(
            args.file, args.line,
            enable_interprocedural=not args.no_interprocedural,
            max_call_depth=args.max_call_depth,
            verbose=True  # å•æ–‡ä»¶æ¨¡å¼å§‹ç»ˆæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        )
        
        # ç®€åŒ–ç»“æœï¼šç§»é™¤å†—ä½™å­—æ®µ
        simplified_result = {k: v for k, v in result.items() 
                           if k not in ['function_map', 'slice_code_lines', 'analysis_info']}
        
        # ä¿å­˜ç®€åŒ–åçš„ç»“æœ
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(simplified_result, f, indent=2, ensure_ascii=False)
        
        # åŒæ—¶ä¿å­˜é‡æ„ä»£ç åˆ°å•ç‹¬æ–‡ä»¶
        if result.get('reconstruction_success'):
            code_output = args.output.replace('.json', '_reconstructed.c')
            with open(code_output, 'w', encoding='utf-8') as f:
                f.write(result['reconstructed_code'])
            print(f"é‡æ„ä»£ç å·²ä¿å­˜åˆ°: {code_output}")
        
        print(f"å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        
    else:
        # æ‰¹é‡æ¨¡å¼
        print("=" * 60)
        print("æ‰¹é‡åˆ‡ç‰‡ + é‡æ„æ¨¡å¼")
        print("=" * 60)
        
        process_dataset(
            args.data_file, args.output,
            enable_interprocedural=not args.no_interprocedural,
            max_call_depth=args.max_call_depth,
            max_samples=args.max_samples,
            resume=not args.no_resume,
            num_workers=args.workers,
            save_interval=args.save_interval,
            use_processes=not args.use_threads,
            chunk_size=args.chunk_size
        )


if __name__ == '__main__':
    main()
